[0m03:43:44.822424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cfb0a1ff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cf8dd7cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cf8dd7d60>]}


============================== 03:43:44.824704 | e9760c39-18b2-441b-a57b-24278b07d14c ==============================
[0m03:43:44.824704 [info ] [MainThread]: Running with dbt=1.6.10
[0m03:43:44.829560 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/dbt-files', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/jaffle_shop_snowpark/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt compile --output json --select customers --profile DEFAULT --profiles-dir /mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/dbt-files', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m03:43:45.875739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e9760c39-18b2-441b-a57b-24278b07d14c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cf8dd79a0>]}
[0m03:43:45.894650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e9760c39-18b2-441b-a57b-24278b07d14c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ce627c9d0>]}
[0m03:43:45.898506 [info ] [MainThread]: Registered adapter: databricks=1.6.8
[0m03:43:45.914862 [debug] [MainThread]: checksum: cb2707219def04818abb4a7bd5cbef5cb0d33b4ec9a7321738cb92f485b03034, vars: {}, profile: DEFAULT, target: , version: 1.6.10
[0m03:43:45.921612 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m03:43:45.928863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e9760c39-18b2-441b-a57b-24278b07d14c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ce628c880>]}
[0m03:43:47.251764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e9760c39-18b2-441b-a57b-24278b07d14c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ce5e67f40>]}
[0m03:43:47.359741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e9760c39-18b2-441b-a57b-24278b07d14c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ce627d270>]}
[0m03:43:47.364054 [info ] [MainThread]: Found 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 476 macros, 0 groups, 0 semantic models
[0m03:43:47.368129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e9760c39-18b2-441b-a57b-24278b07d14c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ce5ece170>]}
[0m03:43:47.377382 [info ] [MainThread]: 
[0m03:43:47.381461 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m03:43:47.386655 [debug] [ThreadPool]: Acquiring new databricks connection 'list_5x_dev_Anurag_Dev_Databricks_Python'
[0m03:43:47.396888 [debug] [ThreadPool]: Using databricks connection "list_5x_dev_Anurag_Dev_Databricks_Python"
[0m03:43:47.403662 [debug] [ThreadPool]: On list_5x_dev_Anurag_Dev_Databricks_Python: GetTables(database=5x_dev, schema=Anurag_Dev_Databricks_Python, identifier=None)
[0m03:43:47.409107 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:45:13.850389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a304dbe80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a2e07d8a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a2e07d840>]}


============================== 03:45:13.852872 | 67e0ad55-dc5a-4bc6-a0fc-50f18bfbb505 ==============================
[0m03:45:13.852872 [info ] [MainThread]: Running with dbt=1.6.10
[0m03:45:13.857716 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/jaffle_shop_snowpark/logs', 'profiles_dir': '/mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/dbt-files', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt compile --output json --select stg_customers --profile DEFAULT --profiles-dir /mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/dbt-files', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m03:45:14.875577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '67e0ad55-dc5a-4bc6-a0fc-50f18bfbb505', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a32825e10>]}
[0m03:45:14.894843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '67e0ad55-dc5a-4bc6-a0fc-50f18bfbb505', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a1b508070>]}
[0m03:45:14.900344 [info ] [MainThread]: Registered adapter: databricks=1.6.8
[0m03:45:14.918812 [debug] [MainThread]: checksum: cb2707219def04818abb4a7bd5cbef5cb0d33b4ec9a7321738cb92f485b03034, vars: {}, profile: DEFAULT, target: , version: 1.6.10
[0m03:45:14.977369 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:45:14.981161 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:45:14.994694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67e0ad55-dc5a-4bc6-a0fc-50f18bfbb505', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a1b1a40d0>]}
[0m03:45:15.118423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67e0ad55-dc5a-4bc6-a0fc-50f18bfbb505', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a1b363d30>]}
[0m03:45:15.122179 [info ] [MainThread]: Found 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 476 macros, 0 groups, 0 semantic models
[0m03:45:15.126878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67e0ad55-dc5a-4bc6-a0fc-50f18bfbb505', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a1b363ee0>]}
[0m03:45:15.133164 [info ] [MainThread]: 
[0m03:45:15.142031 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m03:45:15.146741 [debug] [ThreadPool]: Acquiring new databricks connection 'list_5x_dev_Anurag_Dev_Databricks_Python'
[0m03:45:15.156519 [debug] [ThreadPool]: Using databricks connection "list_5x_dev_Anurag_Dev_Databricks_Python"
[0m03:45:15.161527 [debug] [ThreadPool]: On list_5x_dev_Anurag_Dev_Databricks_Python: GetTables(database=5x_dev, schema=Anurag_Dev_Databricks_Python, identifier=None)
[0m03:45:15.166409 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:45:38.939466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e40317d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e3df05960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e3df05900>]}


============================== 03:45:38.941891 | 1ac185fc-a450-409d-a082-6ab59b7c9475 ==============================
[0m03:45:38.941891 [info ] [MainThread]: Running with dbt=1.6.10
[0m03:45:38.945868 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/dbt-files', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/jaffle_shop_snowpark/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt compile --output json --select stg_customers --profile DEFAULT --profiles-dir /mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/dbt-files', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m03:45:39.967311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ac185fc-a450-409d-a082-6ab59b7c9475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e4265de10>]}
[0m03:45:39.986173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ac185fc-a450-409d-a082-6ab59b7c9475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e2b4500a0>]}
[0m03:45:39.990437 [info ] [MainThread]: Registered adapter: databricks=1.6.8
[0m03:45:40.005939 [debug] [MainThread]: checksum: cb2707219def04818abb4a7bd5cbef5cb0d33b4ec9a7321738cb92f485b03034, vars: {}, profile: DEFAULT, target: , version: 1.6.10
[0m03:45:40.061120 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:45:40.065865 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:45:40.076514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ac185fc-a450-409d-a082-6ab59b7c9475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e2b0d80d0>]}
[0m03:45:40.197073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ac185fc-a450-409d-a082-6ab59b7c9475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e2b09fe80>]}
[0m03:45:40.202181 [info ] [MainThread]: Found 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 476 macros, 0 groups, 0 semantic models
[0m03:45:40.206883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ac185fc-a450-409d-a082-6ab59b7c9475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e2b09ff10>]}
[0m03:45:40.212040 [info ] [MainThread]: 
[0m03:45:40.218743 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m03:45:40.223389 [debug] [ThreadPool]: Acquiring new databricks connection 'list_5x_dev_Anurag_Dev_Databricks_Python'
[0m03:45:40.231177 [debug] [ThreadPool]: Using databricks connection "list_5x_dev_Anurag_Dev_Databricks_Python"
[0m03:45:40.235730 [debug] [ThreadPool]: On list_5x_dev_Anurag_Dev_Databricks_Python: GetTables(database=5x_dev, schema=Anurag_Dev_Databricks_Python, identifier=None)
[0m03:45:40.240760 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:47:20.340053 [debug] [ThreadPool]: SQL status: OK in 100.0999984741211 seconds
[0m03:47:20.347598 [debug] [ThreadPool]: On list_5x_dev_Anurag_Dev_Databricks_Python: Close
[0m03:47:20.577632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ac185fc-a450-409d-a082-6ab59b7c9475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e2b09f370>]}
[0m03:47:20.582352 [info ] [MainThread]: Concurrency: 1 threads (target='default')
[0m03:47:20.587059 [info ] [MainThread]: 
[0m03:47:20.603474 [debug] [Thread-1 (]: Began running node model.jaffle_shop_snowpark.stg_customers
[0m03:47:20.607543 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_5x_dev_Anurag_Dev_Databricks_Python, now model.jaffle_shop_snowpark.stg_customers)
[0m03:47:20.612713 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop_snowpark.stg_customers
[0m03:47:20.642104 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop_snowpark.stg_customers"
[0m03:47:20.671039 [debug] [Thread-1 (]: Timing info for model.jaffle_shop_snowpark.stg_customers (compile): 03:47:20.616212 => 03:47:20.670804
[0m03:47:20.675883 [debug] [Thread-1 (]: Began executing node model.jaffle_shop_snowpark.stg_customers
[0m03:47:20.680441 [debug] [Thread-1 (]: Timing info for model.jaffle_shop_snowpark.stg_customers (execute): 03:47:20.680217 => 03:47:20.680236
[0m03:47:20.685246 [debug] [Thread-1 (]: Finished running node model.jaffle_shop_snowpark.stg_customers
[0m03:47:20.689978 [debug] [Thread-1 (]: Began running node test.jaffle_shop_snowpark.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:47:20.694770 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop_snowpark.stg_customers, now test.jaffle_shop_snowpark.not_null_stg_customers_customer_id.e2cfb1f9aa)
[0m03:47:20.700461 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop_snowpark.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:47:20.715789 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop_snowpark.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m03:47:20.735810 [debug] [Thread-1 (]: Timing info for test.jaffle_shop_snowpark.not_null_stg_customers_customer_id.e2cfb1f9aa (compile): 03:47:20.704603 => 03:47:20.735569
[0m03:47:20.739833 [debug] [Thread-1 (]: Began executing node test.jaffle_shop_snowpark.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:47:20.744327 [debug] [Thread-1 (]: Timing info for test.jaffle_shop_snowpark.not_null_stg_customers_customer_id.e2cfb1f9aa (execute): 03:47:20.744113 => 03:47:20.744124
[0m03:47:20.749609 [debug] [Thread-1 (]: Finished running node test.jaffle_shop_snowpark.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m03:47:20.753279 [debug] [Thread-1 (]: Began running node test.jaffle_shop_snowpark.unique_stg_customers_customer_id.c7614daada
[0m03:47:20.757218 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.jaffle_shop_snowpark.not_null_stg_customers_customer_id.e2cfb1f9aa, now test.jaffle_shop_snowpark.unique_stg_customers_customer_id.c7614daada)
[0m03:47:20.761706 [debug] [Thread-1 (]: Began compiling node test.jaffle_shop_snowpark.unique_stg_customers_customer_id.c7614daada
[0m03:47:20.772794 [debug] [Thread-1 (]: Writing injected SQL for node "test.jaffle_shop_snowpark.unique_stg_customers_customer_id.c7614daada"
[0m03:47:20.789310 [debug] [Thread-1 (]: Timing info for test.jaffle_shop_snowpark.unique_stg_customers_customer_id.c7614daada (compile): 03:47:20.765928 => 03:47:20.789101
[0m03:47:20.800173 [debug] [Thread-1 (]: Began executing node test.jaffle_shop_snowpark.unique_stg_customers_customer_id.c7614daada
[0m03:47:20.804669 [debug] [Thread-1 (]: Timing info for test.jaffle_shop_snowpark.unique_stg_customers_customer_id.c7614daada (execute): 03:47:20.804468 => 03:47:20.804478
[0m03:47:20.809654 [debug] [Thread-1 (]: Finished running node test.jaffle_shop_snowpark.unique_stg_customers_customer_id.c7614daada
[0m03:47:20.814070 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:47:20.818511 [debug] [MainThread]: Connection 'test.jaffle_shop_snowpark.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m03:47:20.823305 [debug] [MainThread]: Command end result
[0m03:47:20.921894 [debug] [MainThread]: Excluded node 'not_null_stg_customers_customer_id' from results
[0m03:47:20.926751 [debug] [MainThread]: Excluded node 'unique_stg_customers_customer_id' from results
[0m03:47:20.932961 [info ] [MainThread]: {
  "node": "stg_customers",
  "compiled": "import snowflake.snowpark.functions as f\nfrom snowflake.snowpark.functions import col\n\ndef model(dbt, session):\n    dbt.config(materialized = \"table\")\n    df = dbt.ref(\"raw_customers\")\n    df_new= df.select(df[\"id\"].alias(\"customer_id\"), df.first_name, df.last_name)\n    return df_new\n\n\n# This part is user provided model code\n# you will need to copy the next section to run the code\n# COMMAND ----------\n# this part is dbt logic for get ref work, do not modify\n\ndef ref(*args, **kwargs):\n    refs = {\"raw_customers\": \"`5x_dev`.`Anurag_Dev_Databricks_Python`.`raw_customers`\"}\n    key = '.'.join(args)\n    version = kwargs.get(\"v\") or kwargs.get(\"version\")\n    if version:\n        key += f\".v{version}\"\n    dbt_load_df_function = kwargs.get(\"dbt_load_df_function\")\n    return dbt_load_df_function(refs[key])\n\n\ndef source(*args, dbt_load_df_function):\n    sources = {}\n    key = '.'.join(args)\n    return dbt_load_df_function(sources[key])\n\n\nconfig_dict = {}\n\n\nclass config:\n    def __init__(self, *args, **kwargs):\n        pass\n\n    @staticmethod\n    def get(key, default=None):\n        return config_dict.get(key, default)\n\nclass this:\n    \"\"\"dbt.this() or dbt.this.identifier\"\"\"\n    database = \"5x_dev\"\n    schema = \"Anurag_Dev_Databricks_Python\"\n    identifier = \"stg_customers\"\n    \n    def __repr__(self):\n        return '`5x_dev`.`Anurag_Dev_Databricks_Python`.`stg_customers`'\n\n\nclass dbtObj:\n    def __init__(self, load_df_function) -> None:\n        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)\n        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)\n        self.config = config\n        self.this = this()\n        self.is_incremental = False\n\n# COMMAND ----------\n\n# how to execute python model in notebook\n# dbt = dbtObj(spark.table)\n# df = model(dbt, spark)\n\n"
}
[0m03:47:20.939397 [debug] [MainThread]: Command `dbt compile` succeeded at 03:47:20.939284 after 102.01 seconds
[0m03:47:20.948321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e40317d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e2afd5990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7e2b0a8160>]}
[0m03:47:20.952974 [debug] [MainThread]: Flushing usage events
[0m04:21:34.107763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf685c3dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf661a9ae0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf661a9a80>]}


============================== 04:21:34.110281 | 9f4c8329-c0d2-45ce-b3c5-1052d50ea83c ==============================
[0m04:21:34.110281 [info ] [MainThread]: Running with dbt=1.6.10
[0m04:21:34.116005 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/dbt-files', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/jaffle_shop_snowpark/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --profiles-dir /mnt/efs/fs1/dbt-core-dev/427e1d61-9912-4142-8bbe-5220cf80f1cd/805a1d64-6c0c-4682-be0b-a1ea4080d49b/12de3295-704c-4a52-98ba-9574115f2c32/dbt-files --profile DEFAULT', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m04:21:35.135690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9f4c8329-c0d2-45ce-b3c5-1052d50ea83c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf6a903250>]}
[0m04:21:35.155347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9f4c8329-c0d2-45ce-b3c5-1052d50ea83c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf536a8370>]}
[0m04:21:35.161194 [info ] [MainThread]: Registered adapter: databricks=1.6.8
[0m04:21:35.185264 [debug] [MainThread]: checksum: cb2707219def04818abb4a7bd5cbef5cb0d33b4ec9a7321738cb92f485b03034, vars: {}, profile: DEFAULT, target: , version: 1.6.10
[0m04:21:35.277180 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m04:21:35.286505 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m04:21:35.297240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9f4c8329-c0d2-45ce-b3c5-1052d50ea83c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf533480d0>]}
[0m04:21:35.461751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9f4c8329-c0d2-45ce-b3c5-1052d50ea83c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf534ffeb0>]}
[0m04:21:35.466987 [info ] [MainThread]: Found 5 models, 3 seeds, 20 tests, 0 sources, 0 exposures, 0 metrics, 476 macros, 0 groups, 0 semantic models
[0m04:21:35.471290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f4c8329-c0d2-45ce-b3c5-1052d50ea83c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf53318430>]}
[0m04:21:35.482223 [info ] [MainThread]: 
[0m04:21:35.487934 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m04:21:35.498652 [debug] [ThreadPool]: Acquiring new databricks connection 'list_5x_dev'
[0m04:21:35.506806 [debug] [ThreadPool]: Using databricks connection "list_5x_dev"
[0m04:21:35.513320 [debug] [ThreadPool]: On list_5x_dev: GetSchemas(database=5x_dev, schema=None)
[0m04:21:35.521743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:21:37.681411 [debug] [ThreadPool]: SQL status: OK in 2.1600000858306885 seconds
[0m04:21:37.692051 [debug] [ThreadPool]: On list_5x_dev: Close
[0m04:21:37.947091 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_5x_dev, now create_5x_dev_Anurag_Dev_Databricks_Python)
[0m04:21:37.953352 [debug] [ThreadPool]: Creating schema "database: "5x_dev"
schema: "Anurag_Dev_Databricks_Python"
"
[0m04:21:37.967638 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m04:21:37.972349 [debug] [ThreadPool]: Using databricks connection "create_5x_dev_Anurag_Dev_Databricks_Python"
[0m04:21:37.977750 [debug] [ThreadPool]: On create_5x_dev_Anurag_Dev_Databricks_Python: /* {"app": "dbt", "dbt_version": "1.6.10", "dbt_databricks_version": "1.6.8", "databricks_sql_connector_version": "2.9.4", "profile_name": "DEFAULT", "target_name": "default", "connection_name": "create_5x_dev_Anurag_Dev_Databricks_Python"} */
create schema if not exists `5x_dev`.`Anurag_Dev_Databricks_Python`
  
[0m04:21:38.007049 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:21:45.392883 [debug] [ThreadPool]: SQL status: OK in 7.389999866485596 seconds
[0m04:21:45.691397 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m04:21:45.697265 [debug] [ThreadPool]: On create_5x_dev_Anurag_Dev_Databricks_Python: ROLLBACK
[0m04:21:45.702397 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m04:21:45.711142 [debug] [ThreadPool]: On create_5x_dev_Anurag_Dev_Databricks_Python: Close
[0m04:21:45.944668 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_5x_dev_Anurag_Dev_Databricks_Python, now list_5x_dev_Anurag_Dev_Databricks_Python)
[0m04:21:45.955449 [debug] [ThreadPool]: Using databricks connection "list_5x_dev_Anurag_Dev_Databricks_Python"
[0m04:21:45.960408 [debug] [ThreadPool]: On list_5x_dev_Anurag_Dev_Databricks_Python: GetTables(database=5x_dev, schema=Anurag_Dev_Databricks_Python, identifier=None)
[0m04:21:45.969213 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m04:21:47.429874 [debug] [ThreadPool]: SQL status: OK in 1.4600000381469727 seconds
[0m04:21:47.440645 [debug] [ThreadPool]: On list_5x_dev_Anurag_Dev_Databricks_Python: Close
[0m04:21:47.676381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f4c8329-c0d2-45ce-b3c5-1052d50ea83c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf533ef250>]}
[0m04:21:47.684621 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:21:47.692112 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:21:47.696958 [info ] [MainThread]: Concurrency: 1 threads (target='default')
[0m04:21:47.705355 [info ] [MainThread]: 
[0m04:21:47.714701 [debug] [Thread-1 (]: Began running node model.jaffle_shop_snowpark.stg_customers
[0m04:21:47.721639 [info ] [Thread-1 (]: 1 of 5 START python table model Anurag_Dev_Databricks_Python.stg_customers ..... [RUN]
[0m04:21:47.733633 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_5x_dev_Anurag_Dev_Databricks_Python, now model.jaffle_shop_snowpark.stg_customers)
[0m04:21:47.744033 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop_snowpark.stg_customers
[0m04:21:47.777025 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop_snowpark.stg_customers"
[0m04:21:47.836307 [debug] [Thread-1 (]: Timing info for model.jaffle_shop_snowpark.stg_customers (compile): 04:21:47.751917 => 04:21:47.836070
[0m04:21:47.841638 [debug] [Thread-1 (]: Began executing node model.jaffle_shop_snowpark.stg_customers
[0m04:21:47.875830 [debug] [Thread-1 (]: Writing runtime python for node "model.jaffle_shop_snowpark.stg_customers"
[0m04:21:47.924826 [debug] [Thread-1 (]: On model.jaffle_shop_snowpark.stg_customers: 
  
    
import snowflake.snowpark.functions as f
from snowflake.snowpark.functions import col

def model(dbt, session):
    dbt.config(materialized = "table")
    df = dbt.ref("raw_customers")
    df_new= df.select(df["id"].alias("customer_id"), df.first_name, df.last_name)
    return df_new


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {"raw_customers": "`5x_dev`.`Anurag_Dev_Databricks_Python`.`raw_customers`"}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "5x_dev"
    schema = "Anurag_Dev_Databricks_Python"
    identifier = "stg_customers"
    
    def __repr__(self):
        return '`5x_dev`.`Anurag_Dev_Databricks_Python`.`stg_customers`'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

import pyspark

# make sure pyspark.sql.connect.dataframe exists before using it
try:
    import pyspark.sql.connect.dataframe
    newer_pyspark_available = True
except ImportError:
    newer_pyspark_available = False

# make sure pandas exists before using it
try:
    import pandas
    pandas_available = True
except ImportError:
    pandas_available = False

# make sure pyspark.pandas exists before using it
try:
    import pyspark.pandas
    pyspark_pandas_api_available = True
except ImportError:
    pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
    import databricks.koalas
    koalas_available = True
except ImportError:
    koalas_available = False


# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
  
if pandas_available and isinstance(df, pandas.core.frame.DataFrame):
    if pyspark_pandas_api_available:
        df = pyspark.pandas.frame.DataFrame(df)
    elif koalas_available:
        df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
    pass  # since it is already a Spark DataFrame
elif newer_pyspark_available and isinstance(df, pyspark.sql.connect.dataframe.DataFrame):
    pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
    df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
    df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
    df = spark.createDataFrame(df)
else:
    msg = f"{type(df)} is not a supported type for dbt Python materialization"
    raise Exception(msg)

writer = (
    df.write
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .format("delta")
)

writer.saveAsTable("`5x_dev`.`Anurag_Dev_Databricks_Python`.`stg_customers`")

  
[0m04:21:47.930184 [debug] [Thread-1 (]: Timing info for model.jaffle_shop_snowpark.stg_customers (execute): 04:21:47.846527 => 04:21:47.929976
[0m04:21:47.934860 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/jaffle_shop_snowpark/models/staging/stg_customers.py[0m
Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.
[0m04:21:47.941149 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/base.py", line 372, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/base.py", line 323, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/base.py", line 419, in run
    return self.execute(compiled_node, manifest)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/run.py", line 291, in execute
    result = MacroGenerator(
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/context/providers.py", line 1338, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 150, in execution_with_log
    response = code_execution_function(*args)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1287, in submit_python_job
    job_helper = self.python_submission_helpers[submission_method](
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/databricks/python_submissions.py", line 410, in __init__
    super().__init__(
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/databricks/python_submissions.py", line 34, in __init__
    self.check_credentials()
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/databricks/python_submissions.py", line 475, in check_credentials
    raise ValueError(
ValueError: Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.

[0m04:21:47.947850 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f4c8329-c0d2-45ce-b3c5-1052d50ea83c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf533feaa0>]}
[0m04:21:47.953227 [error] [Thread-1 (]: 1 of 5 ERROR creating python table model Anurag_Dev_Databricks_Python.stg_customers  [[31mERROR[0m in 0.21s]
[0m04:21:47.960094 [debug] [Thread-1 (]: Finished running node model.jaffle_shop_snowpark.stg_customers
[0m04:21:47.965490 [debug] [Thread-1 (]: Began running node model.jaffle_shop_snowpark.stg_orders
[0m04:21:47.972082 [info ] [Thread-1 (]: 2 of 5 START python table model Anurag_Dev_Databricks_Python.stg_orders ........ [RUN]
[0m04:21:47.977701 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop_snowpark.stg_customers, now model.jaffle_shop_snowpark.stg_orders)
[0m04:21:47.982336 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop_snowpark.stg_orders
[0m04:21:47.990914 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop_snowpark.stg_orders"
[0m04:21:48.005667 [debug] [Thread-1 (]: Timing info for model.jaffle_shop_snowpark.stg_orders (compile): 04:21:47.987389 => 04:21:48.005457
[0m04:21:48.011427 [debug] [Thread-1 (]: Began executing node model.jaffle_shop_snowpark.stg_orders
[0m04:21:48.018648 [debug] [Thread-1 (]: Writing runtime python for node "model.jaffle_shop_snowpark.stg_orders"
[0m04:21:48.034905 [debug] [Thread-1 (]: On model.jaffle_shop_snowpark.stg_orders: 
  
    
import snowflake.snowpark.functions as f
from snowflake.snowpark.functions import col

def model(dbt, session):
    dbt.config(materialized = "table")
    df = dbt.ref("raw_orders")
    df_new = df.select(df["id"].alias("order_id"),df["user_id"].alias("customer_id"), col("order_date"), col("status"))
    return df_new


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {"raw_orders": "`5x_dev`.`Anurag_Dev_Databricks_Python`.`raw_orders`"}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "5x_dev"
    schema = "Anurag_Dev_Databricks_Python"
    identifier = "stg_orders"
    
    def __repr__(self):
        return '`5x_dev`.`Anurag_Dev_Databricks_Python`.`stg_orders`'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

import pyspark

# make sure pyspark.sql.connect.dataframe exists before using it
try:
    import pyspark.sql.connect.dataframe
    newer_pyspark_available = True
except ImportError:
    newer_pyspark_available = False

# make sure pandas exists before using it
try:
    import pandas
    pandas_available = True
except ImportError:
    pandas_available = False

# make sure pyspark.pandas exists before using it
try:
    import pyspark.pandas
    pyspark_pandas_api_available = True
except ImportError:
    pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
    import databricks.koalas
    koalas_available = True
except ImportError:
    koalas_available = False


# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
  
if pandas_available and isinstance(df, pandas.core.frame.DataFrame):
    if pyspark_pandas_api_available:
        df = pyspark.pandas.frame.DataFrame(df)
    elif koalas_available:
        df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
    pass  # since it is already a Spark DataFrame
elif newer_pyspark_available and isinstance(df, pyspark.sql.connect.dataframe.DataFrame):
    pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
    df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
    df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
    df = spark.createDataFrame(df)
else:
    msg = f"{type(df)} is not a supported type for dbt Python materialization"
    raise Exception(msg)

writer = (
    df.write
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .format("delta")
)

writer.saveAsTable("`5x_dev`.`Anurag_Dev_Databricks_Python`.`stg_orders`")

  
[0m04:21:48.042731 [debug] [Thread-1 (]: Timing info for model.jaffle_shop_snowpark.stg_orders (execute): 04:21:48.015756 => 04:21:48.042531
[0m04:21:48.051660 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/jaffle_shop_snowpark/models/staging/stg_orders.py[0m
Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.
[0m04:21:48.057369 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/base.py", line 372, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/base.py", line 323, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/base.py", line 419, in run
    return self.execute(compiled_node, manifest)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/run.py", line 291, in execute
    result = MacroGenerator(
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/context/providers.py", line 1338, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 150, in execution_with_log
    response = code_execution_function(*args)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1287, in submit_python_job
    job_helper = self.python_submission_helpers[submission_method](
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/databricks/python_submissions.py", line 410, in __init__
    super().__init__(
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/databricks/python_submissions.py", line 34, in __init__
    self.check_credentials()
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/databricks/python_submissions.py", line 475, in check_credentials
    raise ValueError(
ValueError: Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.

[0m04:21:48.062560 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f4c8329-c0d2-45ce-b3c5-1052d50ea83c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf533fc8e0>]}
[0m04:21:48.068203 [error] [Thread-1 (]: 2 of 5 ERROR creating python table model Anurag_Dev_Databricks_Python.stg_orders  [[31mERROR[0m in 0.09s]
[0m04:21:48.073133 [debug] [Thread-1 (]: Finished running node model.jaffle_shop_snowpark.stg_orders
[0m04:21:48.079585 [debug] [Thread-1 (]: Began running node model.jaffle_shop_snowpark.stg_payments
[0m04:21:48.084620 [info ] [Thread-1 (]: 3 of 5 START python table model Anurag_Dev_Databricks_Python.stg_payments ...... [RUN]
[0m04:21:48.092442 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jaffle_shop_snowpark.stg_orders, now model.jaffle_shop_snowpark.stg_payments)
[0m04:21:48.097513 [debug] [Thread-1 (]: Began compiling node model.jaffle_shop_snowpark.stg_payments
[0m04:21:48.109536 [debug] [Thread-1 (]: Writing injected SQL for node "model.jaffle_shop_snowpark.stg_payments"
[0m04:21:48.126098 [debug] [Thread-1 (]: Timing info for model.jaffle_shop_snowpark.stg_payments (compile): 04:21:48.106102 => 04:21:48.125890
[0m04:21:48.131339 [debug] [Thread-1 (]: Began executing node model.jaffle_shop_snowpark.stg_payments
[0m04:21:48.139414 [debug] [Thread-1 (]: Writing runtime python for node "model.jaffle_shop_snowpark.stg_payments"
[0m04:21:48.159095 [debug] [Thread-1 (]: On model.jaffle_shop_snowpark.stg_payments: 
  
    
import snowflake.snowpark.functions as f
from snowflake.snowpark.functions import col

def model(dbt, session):
    dbt.config(materialized = "table")
    df = dbt.ref("raw_payments")
    df_new= df.select(df["id"].alias("payment_id"), col("order_id"), col("payment_method"), col("amount")).with_column("amount", df["amount"] / 100)
    return df_new


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args, **kwargs):
    refs = {"raw_payments": "`5x_dev`.`Anurag_Dev_Databricks_Python`.`raw_payments`"}
    key = '.'.join(args)
    version = kwargs.get("v") or kwargs.get("version")
    if version:
        key += f".v{version}"
    dbt_load_df_function = kwargs.get("dbt_load_df_function")
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = '.'.join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = "5x_dev"
    schema = "Anurag_Dev_Databricks_Python"
    identifier = "stg_payments"
    
    def __repr__(self):
        return '`5x_dev`.`Anurag_Dev_Databricks_Python`.`stg_payments`'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args, **kwargs: ref(*args, **kwargs, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

import pyspark

# make sure pyspark.sql.connect.dataframe exists before using it
try:
    import pyspark.sql.connect.dataframe
    newer_pyspark_available = True
except ImportError:
    newer_pyspark_available = False

# make sure pandas exists before using it
try:
    import pandas
    pandas_available = True
except ImportError:
    pandas_available = False

# make sure pyspark.pandas exists before using it
try:
    import pyspark.pandas
    pyspark_pandas_api_available = True
except ImportError:
    pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
    import databricks.koalas
    koalas_available = True
except ImportError:
    koalas_available = False


# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
  
if pandas_available and isinstance(df, pandas.core.frame.DataFrame):
    if pyspark_pandas_api_available:
        df = pyspark.pandas.frame.DataFrame(df)
    elif koalas_available:
        df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
    pass  # since it is already a Spark DataFrame
elif newer_pyspark_available and isinstance(df, pyspark.sql.connect.dataframe.DataFrame):
    pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
    df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
    df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
    df = spark.createDataFrame(df)
else:
    msg = f"{type(df)} is not a supported type for dbt Python materialization"
    raise Exception(msg)

writer = (
    df.write
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .format("delta")
)

writer.saveAsTable("`5x_dev`.`Anurag_Dev_Databricks_Python`.`stg_payments`")

  
[0m04:21:48.164007 [debug] [Thread-1 (]: Timing info for model.jaffle_shop_snowpark.stg_payments (execute): 04:21:48.136740 => 04:21:48.163801
[0m04:21:48.173816 [error] [Thread-1 (]: [31mUnhandled error while executing target/run/jaffle_shop_snowpark/models/staging/stg_payments.py[0m
Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.
[0m04:21:48.179573 [debug] [Thread-1 (]: Traceback (most recent call last):
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/base.py", line 372, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/base.py", line 323, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/base.py", line 419, in run
    return self.execute(compiled_node, manifest)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/task/run.py", line 291, in execute
    result = MacroGenerator(
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/clients/jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 55, in macro
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/jinja2/runtime.py", line 298, in call
    return __obj(*args, **kwargs)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/context/providers.py", line 1338, in submit_python_job
    return self.adapter.submit_python_job(parsed_model, compiled_code)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 150, in execution_with_log
    response = code_execution_function(*args)
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/base/impl.py", line 1287, in submit_python_job
    job_helper = self.python_submission_helpers[submission_method](
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/databricks/python_submissions.py", line 410, in __init__
    super().__init__(
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/databricks/python_submissions.py", line 34, in __init__
    self.check_credentials()
  File "/root/dbt-1.6/lib/python3.10/site-packages/dbt/adapters/databricks/python_submissions.py", line 475, in check_credentials
    raise ValueError(
ValueError: Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.

[0m04:21:48.185050 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9f4c8329-c0d2-45ce-b3c5-1052d50ea83c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf532ff040>]}
[0m04:21:48.191383 [error] [Thread-1 (]: 3 of 5 ERROR creating python table model Anurag_Dev_Databricks_Python.stg_payments  [[31mERROR[0m in 0.09s]
[0m04:21:48.196890 [debug] [Thread-1 (]: Finished running node model.jaffle_shop_snowpark.stg_payments
[0m04:21:48.201534 [debug] [Thread-1 (]: Began running node model.jaffle_shop_snowpark.customers
[0m04:21:48.206925 [info ] [Thread-1 (]: 4 of 5 SKIP relation Anurag_Dev_Databricks_Python.customers .................... [[33mSKIP[0m]
[0m04:21:48.212408 [debug] [Thread-1 (]: Finished running node model.jaffle_shop_snowpark.customers
[0m04:21:48.217527 [debug] [Thread-1 (]: Began running node model.jaffle_shop_snowpark.orders
[0m04:21:48.222002 [info ] [Thread-1 (]: 5 of 5 SKIP relation Anurag_Dev_Databricks_Python.orders ....................... [[33mSKIP[0m]
[0m04:21:48.226795 [debug] [Thread-1 (]: Finished running node model.jaffle_shop_snowpark.orders
[0m04:21:48.232782 [debug] [MainThread]: On master: ROLLBACK
[0m04:21:48.242268 [debug] [MainThread]: Opening a new connection, currently in state init
[0m04:21:49.026003 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m04:21:49.030531 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m04:21:49.040332 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m04:21:49.045552 [debug] [MainThread]: On master: ROLLBACK
[0m04:21:49.049905 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m04:21:49.066215 [debug] [MainThread]: On master: Close
[0m04:21:49.282925 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:21:49.291354 [debug] [MainThread]: Connection 'model.jaffle_shop_snowpark.stg_payments' was properly closed.
[0m04:21:49.296970 [info ] [MainThread]: 
[0m04:21:49.302118 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 13.81 seconds (13.81s).
[0m04:21:49.306891 [debug] [MainThread]: Command end result
[0m04:21:49.411343 [info ] [MainThread]: 
[0m04:21:49.418453 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m04:21:49.424330 [info ] [MainThread]: 
[0m04:21:49.429589 [error] [MainThread]:   Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.
[0m04:21:49.435331 [info ] [MainThread]: 
[0m04:21:49.440341 [error] [MainThread]:   Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.
[0m04:21:49.445663 [info ] [MainThread]: 
[0m04:21:49.450758 [error] [MainThread]:   Databricks `http_path` or `cluster_id` of an all-purpose cluster is required for the `all_purpose_cluster` submission method.
[0m04:21:49.455498 [info ] [MainThread]: 
[0m04:21:49.460085 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m04:21:49.465487 [debug] [MainThread]: Command `dbt run` failed at 04:21:49.465376 after 15.38 seconds
[0m04:21:49.473359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf685c3dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf53338160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf53339360>]}
[0m04:21:49.496104 [debug] [MainThread]: Flushing usage events
